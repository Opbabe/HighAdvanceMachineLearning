{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvFYuILDsJyu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Q1 Transformer (10 Points)\n",
        "\n",
        "Implement an encoder + decoder transformer model to predict next sentence using keras API and train it on shakespeare db, i.e. given one sentence from shakespeare text, it should predict next sentence.\n",
        "\n",
        "(Feel free to re-use parts of the colab [transformer intro](https://colab.research.google.com/drive/1ZM-JCeCCzgvxJGiqDliXSRvFDBDKMBTB#scrollTo=cbccHV5HWdOz) done in the class.)\n",
        "\n",
        "Test it on a paragraph (continuous text) with 11 sentences given below:\n",
        "\n",
        "You feed first sentence. Then repeatedly call the model to predict next sentence, feed the output again to predict next sentence and so on to predict 10 sentences. Compare the last predicted sentence with the actual test paragraph's 11th sentence.\n",
        "\n",
        "\n",
        "* Make reasonable assumptions about the model architecture and params.\n",
        "* Build a model from scratch using Keras layers, don't use pretrained models or hugging-face libraries etc.\n",
        "* Use a small word dictionary and smaller embeddings to reduce the number of params of the model. (Don't use character dictionary.)\n",
        "* Use special tokens `<SOS>` (start of sentence) and `<EOS>` (end of sentence).\n",
        "* Use standard positional encoding.\n",
        "\n",
        "\n",
        "Getting the data\n",
        "\n",
        "* [dataset](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays)\n",
        "* You can just use the [alllines.txt](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays?select=alllines.txt).\n",
        "\n",
        "\n",
        "Test paragraph (11 contiguous sentences)\n",
        "\n",
        "* \"Whose end is purposed by the mighty gods?\"\n",
        "* \"Yet Caesar shall go forth, for these predictions\"\n",
        "* \"Are to the world in general as to Caesar.\"\n",
        "* \"When beggars die, there are no comets seen,\"\n",
        "* \"The heavens themselves blaze forth the death of princes.\"\n",
        "* \"Cowards die many times before their deaths,\"\n",
        "* \"The valiant never taste of death but once.\"\n",
        "* \"Of all the wonders that I yet have heard.\"\n",
        "* \"It seems to me most strange that men should fear,\"\n",
        "* \"Seeing that death, a necessary end,\"\n",
        "* \"Will come when it will come.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siunTtHXIjVO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "myAQMM6nsEDf",
        "outputId": "18a3ed78-0222-46df-bb90-e30d0bf73bc5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d75caa1-a54a-4ad6-8df1-5c7b695cf304\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7d75caa1-a54a-4ad6-8df1-5c7b695cf304\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving alllines.txt to alllines.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.rename(\"alllines.txt\", \"alllines.txt\")\n",
        "#Here for our assignment we are only focusing on allines..txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIShCXQVcyd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCQaWOHGU9ST"
      },
      "source": [
        "Issue and Sucess\n",
        "\n",
        "- After 13 tries and so much testing for hours and hours there were so much problem with my data and model\n",
        "\n",
        "\n",
        "Setting up the data: I would say that I added special words, like <start> at the beginning and <end> at the end of each line. This helped the model know where each line starts and stops.\n",
        "\n",
        "Creating the Model\n",
        "\n",
        "\n",
        "I added positional which helps the model understand the position of each word in a sentence. This layer gave me some issues at first because it couldnt handle masks\n",
        "\n",
        "\n",
        "Also, the training data and expected output needed to be exactly the same size, which required careful adjustments to avoid size mismatch errors.\n",
        "Fine-Tuning and Training\n",
        "\n",
        "After all these adjustments, I ran the training, keeping an eye out for any issues in the model accuracy and loss how close the correctness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-5tPh_jXRsM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wrkcMm9EXVJR"
      },
      "outputs": [],
      "source": [
        " #the layer to help the model understand the order of words, as Transformers process all words at once\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#Masking problem and issue I kept hitting error because some layers didn't support masking so i just made quick\n",
        "#adjustment. (Please professor please look into this I am so frustrated with the masking)\n",
        "\n",
        "\n",
        "# I set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Within this whole assignment stack over flow did not help much but I focus on my tokenization from it\n",
        "\n",
        "#Remember we got to load the right set\n",
        "\n",
        "\n",
        "def load_shakespeare_data(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "    sentences = text.strip().split('\\n')\n",
        "\n",
        "\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    return sentences\n",
        "\n",
        "sentences = load_shakespeare_data('alllines.txt')\n",
        "sentences = ['<start> ' + sentence + ' <end>' for sentence in sentences]\n",
        "max_vocab_size = 5000\n",
        "max_seq_len = 50\n",
        "\n",
        "\n",
        "#Some of the things i use from transformer info were confusing\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=max_vocab_size,\n",
        "    output_sequence_length=max_seq_len,\n",
        "    standardize=None\n",
        ")\n",
        "tokenizer.adapt(sentences)\n",
        "sequences = tokenizer(sentences)\n",
        "encoder_input = sequences[:-1]\n",
        "decoder_target = sequences[1:]\n",
        "\n",
        "train_size = int(len(encoder_input) * 0.8)\n",
        "\n",
        "\n",
        "encoder_input_train = encoder_input[:train_size]\n",
        "\n",
        "decoder_target_train = decoder_target[:train_size]\n",
        "\n",
        "encoder_input_val = encoder_input[train_size:]\n",
        "\n",
        "decoder_target_val = decoder_target[train_size:]\n",
        "\n",
        "# I really dont like the issue with padding and sequence\n",
        "# sequences are correctly aligned was crucial for me\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7qjhBfXMXVBq"
      },
      "outputs": [],
      "source": [
        "# positional Encoding layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, max_seq_len=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        pos = np.arange(max_seq_len)[:, np.newaxis]\n",
        "        i = np.arange(embed_dim)[np.newaxis, :]\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
        "        angle_rads = pos * angle_rates\n",
        "\n",
        "        # using google co lab automation code formated for correct\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]\n",
        "        self.pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def call(self, x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "# just a single Transformer block\n",
        "def transformer_block(embed_dim, num_heads, ff_dim, dropout_rate):\n",
        "    inputs = tf.keras.Input(shape=(None, embed_dim))\n",
        "\n",
        "    # I encountered issues with attention masks, so I kept them simple.\n",
        "    attention = tf.keras.layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate\n",
        "    )(inputs, inputs)\n",
        "\n",
        "    attention = tf.keras.layers.Dropout(dropout_rate)(attention)\n",
        "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    # ff network\n",
        "    ffn = tf.keras.layers.Dense(ff_dim, activation='relu')(out1)\n",
        "\n",
        "\n",
        "    ffn = tf.keras.layers.Dense(embed_dim)(ffn)\n",
        "\n",
        "\n",
        "    ffn = tf.keras.layers.Dropout(dropout_rate)(ffn)\n",
        "\n",
        "\n",
        "    out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn)\n",
        "\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=out2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAcqKUtsXUwZ",
        "outputId": "fa5cc0a8-9ebc-4b12-9774-47391c94af3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'positional_encoding_4' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'positional_encoding_5' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py:2311: UserWarning: Seed 42 from outer graph might be getting used by function Dataset_map_permutation, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.\n",
            "  return map_op._map_v2(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m 995/1393\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m5:55\u001b[0m 894ms/step - accuracy: 0.7866 - loss: 5.0705"
          ]
        }
      ],
      "source": [
        "#After attention layers, I used feedforward dayers to refine the information further.\n",
        "\n",
        "\n",
        "#You guys have to rememebr this type of help will allow me to put the attention output so it can pass along better info\n",
        "embed_dim = 64\n",
        "num_heads = 2\n",
        "ff_dim = 256\n",
        "dropout_rate = 0.1\n",
        "num_layers = 2\n",
        "\n",
        "#clean up very quick using google co lab auto\n",
        "vocab_size = tokenizer.vocabulary_size()\n",
        "encoder_inputs = tf.keras.Input(shape=(max_seq_len,), name=\"encoder_inputs\")\n",
        "decoder_inputs = tf.keras.Input(shape=(max_seq_len,), name=\"decoder_inputs\")\n",
        "\n",
        "#from google auto\n",
        "embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, mask_zero=True)\n",
        "\n",
        "\n",
        "#This step took some troubleshooting because positional encoding can interfere with masking, which raised warnings\n",
        "\n",
        "\n",
        "# I stacked transformer blocks on the encoder and decoder inputs. At first, I had some shape mismatches in this part\n",
        "\n",
        "# I compiled the model with a specific loss and optimizer\n",
        "\n",
        "encoder_embedded = embedding(encoder_inputs)\n",
        "encoder_embedded = PositionalEncoding(embed_dim, max_seq_len)(encoder_embedded)\n",
        "\n",
        "decoder_embedded = embedding(decoder_inputs)\n",
        "decoder_embedded = PositionalEncoding(embed_dim, max_seq_len)(decoder_embedded)\n",
        "\n",
        "encoder = encoder_embedded\n",
        "for _ in range(num_layers):\n",
        "    encoder = transformer_block(embed_dim, num_heads, ff_dim, dropout_rate)(encoder)\n",
        "\n",
        "decoder = decoder_embedded\n",
        "for _ in range(num_layers):\n",
        "    decoder = transformer_block(embed_dim, num_heads, ff_dim, dropout_rate)(decoder)\n",
        "outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder)\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], outputs)\n",
        "\n",
        "# fix the model with optimizer and loss function\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "decoder_target_train = tf.pad(decoder_target_train[:, 1:], [[0, 0], [0, 1]])\n",
        "decoder_target_val = tf.pad(decoder_target_val[:, 1:], [[0, 0], [0, 1]])\n",
        "\n",
        "# I had to ensure that the decoder targets were correctly padded to avoid shape mismatches.\n",
        "\n",
        "# Training the model for 5 epochs at first due to timming issue. Second I set it 10\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    [encoder_input_train, encoder_input_train],\n",
        "    decoder_target_train,\n",
        "    validation_data=(\n",
        "        [encoder_input_val, encoder_input_val],\n",
        "        decoder_target_val\n",
        "    ),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=64\n",
        ")\n",
        "#I set up an encoder and a decoder, each with a few layers, just like in the Transformer intro. The encoder processes the input sentence,\n",
        "#while the decoder generates the next sentence based on the encoder’s understanding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbri1PaUXUVK"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, seed_text, max_seq_len):\n",
        "    # I start with the seed text provided.\n",
        "    generated = seed_text\n",
        "\n",
        "    for _ in range(max_seq_len):\n",
        "        token_list = tokenizer([generated])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        predictions = model.predict([token_list, token_list])\n",
        "\n",
        "        predicted_id = np.argmax(predictions[0, -1, :])\n",
        "\n",
        "        predicted_word = tokenizer.get_vocabulary()[predicted_id]\n",
        "\n",
        "        if predicted_word == '<end>' or predicted_word == '':\n",
        "            break\n",
        "\n",
        "        generated += ' ' + predicted_word\n",
        "\n",
        "    return generated.replace('<start> ', '').replace(' <end>', '')\n",
        "\n",
        "\n",
        "#8 time! Still issue!\n",
        "\n",
        "test_paragraph = [\n",
        "    \"Whose end is purposed by the mighty gods?\",\n",
        "    \"Yet Caesar shall go forth, for these predictions\",\n",
        "    \"Are to the world in general as to Caesar.\",\n",
        "    \"When beggars die, there are no comets seen,\",\n",
        "    \"The heavens themselves blaze forth the death of princes.\",\n",
        "    \"Cowards die many times before their deaths,\",\n",
        "    \"The valiant never taste of death but once.\",\n",
        "    \"Of all the wonders that I yet have heard.\",\n",
        "    \"It seems to me most strange that men should fear,\",\n",
        "    \"Seeing that death, a necessary end,\"\n",
        "]\n",
        "# I ran into errors where my input shapes didn't match the model's expected\n",
        "#shapes, especially when using the encoder-decoder structure\n",
        "\n",
        "\n",
        "\n",
        "# My model struggled with accuracy because the training was slow\n",
        "predicted_sentences = []\n",
        "\n",
        "for i, sentence in enumerate(test_paragraph):\n",
        "    generated = generate_text(model, tokenizer, sentence, max_seq_len=20)\n",
        "    predicted_sentences.append(generated)\n",
        "    print(f\"Sentence {i+1}: {generated}\")\n",
        "\n",
        "print(\"\\nActual 11th sentence:\")\n",
        "print(\"Will come when it will come.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oc2M5Bz51NPu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6lSBTJ9bRuZ"
      },
      "source": [
        "This is one of the output for Epoch at 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqFmcUCmbGYf"
      },
      "source": [
        "I already train the model over 10 times!\n",
        "# I already train the model over 10 times!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToaciRSMWG8Y"
      },
      "source": [
        "My output just in case you can't run it:\n",
        "\n",
        "\n",
        "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'positional_encoding' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
        "  warnings.warn(\n",
        "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'positional_encoding_1' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
        "  warnings.warn(\n",
        "Epoch 1/5\n",
        "1393/1393 ━━━━━━━━━━━━━━━━━━━━ 1316s 940ms/step - accuracy: 0.7909 - loss: 4.3526 - val_accuracy: 0.8484 - val_loss: 0.9906\n",
        "Epoch 2/5\n",
        "1393/1393 ━━━━━━━━━━━━━━━━━━━━ 1389s 974ms/step - accuracy: 0.8466 - loss: 0.9823 - val_accuracy: 0.8488 - val_loss: 0.9342\n",
        "Epoch 3/5\n",
        "1393/1393 ━━━━━━━━━━━━━━━━━━━━ 1346s 966ms/step - accuracy: 0.8472 - loss: 0.9421 - val_accuracy: 0.8491 - val_loss: 0.9225\n",
        "Epoch 4/5\n",
        "1393/1393 ━━━━━━━━━━━━━━━━━━━━ 1299s 933ms/step - accuracy: 0.8475 - loss: 0.9309 - val_accuracy: 0.8493 - val_loss: 0.9171\n",
        "Epoch 5/5\n",
        "1393/1393 ━━━━━━━━━━━━━━━━━━━━ 1340s 931ms/step - accuracy: 0.8478 - loss: 0.9245 - val_accuracy: 0.8495 - val_loss: 0.9141\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 545ms/step\n",
        "Sentence 1: Whose end is purposed by the mighty gods?\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step\n",
        "Sentence 2: Yet Caesar shall go forth, for these predictions\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step\n",
        "Sentence 3: Are to the world in general as to Caesar.\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step\n",
        "Sentence 4: When beggars die, there are no comets seen,\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\n",
        "Sentence 5: The heavens themselves blaze forth the death of princes.\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\n",
        "Sentence 6: Cowards die many times before their deaths,\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step\n",
        "Sentence 7: The valiant never taste of death but once.\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step\n",
        "Sentence 8: Of all the wonders that I yet have heard.\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step\n",
        "Sentence 9: It seems to me most strange that men should fear,\n",
        "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step\n",
        "Sentence 10: Seeing that death, a necessary end,\n",
        "\n",
        "Actual 11th sentence:\n",
        "Will come when it will come."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
