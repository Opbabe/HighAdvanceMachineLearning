{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "* Due date is 11/08 midnight.\n",
        "* Please check this original colab often to see if anything changes in the instructions. Last updated: 11/06 11:00am.\n",
        "* Make a copy of this colab, add your code, run all the cells and save output. Submit your .ipynb file with output. Don't submit a link to your colab, you must submit actual .ipynb file with output.\n",
        "* You must train the model and show the test output. Points will be deducted if your model is not trained / produces run time errors. If you try to build a very large model and it doesn't finish training, than you would get points deducted, so make reasonable adjustments to keep the model size under check, do not train very small models, do not train very large models.\n",
        "\n",
        "\n",
        "Q1 RNN (10 Points)\n",
        "\n",
        "Train a GRU classifier for speech-to-text recognition using [dataset](https://www.kaggle.com/datasets/mathurinache/the-lj-speech-dataset) (split this into train/validation).\n",
        "\n",
        "You may want to use [librosa](https://librosa.org/doc/latest/tutorial.html), or [python speech features](https://python-speech-features.readthedocs.io/en/latest/) (there are some diffs between the two).\n",
        "\n",
        "You may want to use Mel-Frequency Cepstral Coefficients (MFCC) as features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wvFYuILDsJyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ObUcfBoIc7C",
        "outputId": "ca74bc29-844d-42f8-ddaa-98f70bd22add"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/CS171 HW 6/archive.zip'\n",
        "extract_path = '/content/LJSpeech'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "os.listdir(extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQmUzxalJpW1",
        "outputId": "2d19a38e-d4de-42ba-fb82-6c3726c99c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LJSpeech-1.1']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa jiwer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, Bidirectional, Dropout, TimeDistributed, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from jiwer import wer\n",
        "\n",
        "#12 times of training\n",
        "\n",
        "#2 succeed and 10 fail\n",
        "\n",
        "#Some of my reference I use was:\n",
        "#https://github.com/rudrajikadra/Speech-Emotion-Recognition-using-Librosa-library-and-MLPClassifier/blob/master/Speech_Emotion_Recognition_Notebook.ipynb\n",
        "#https://stackoverflow.com/questions/61174521/getting-96-mfcc-features-using-python-speech-features\n",
        "#https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
        "\n",
        "\n",
        "#ISSUE!\n",
        "#1. SO throughout this assingment I learn thatmodel cant really understand in so much way. #I think this is a very diffcult focus\n",
        "#Using blank label, that acts like a space so that the model doesn’t get confused when words end.\n",
        "#FOr some reason the tokenizer was adding 1 to all value which I was frustrated on why.\n",
        "\n",
        "\n",
        "\n",
        "#Model Issue:\n",
        "#I decided to go with a setup called Bidirectional GRU layers\n",
        "\n",
        "\n",
        "#Training and Error (Lots of It!)\n",
        "#My labels weren’t lining up with the ml output which  I had to reshape my data to match. I had to fix up the label\n",
        "#lengths and make sure everything match\n",
        "\n",
        "#google co lab auto fill and auto correction was very useless when it comes to token!\n",
        "#WOuld be better if you provided diagram\n",
        "\n",
        "\n",
        "\n",
        "metadata = pd.read_csv('/content/LJSpeech/LJSpeech-1.1/metadata.csv', sep='|', header=None,\n",
        "                       names=['file_name', 'transcription', 'normalized_transcription'])\n",
        "\n",
        "metadata = metadata[:3000]\n",
        "\n",
        "metadata = metadata.dropna(subset=['normalized_transcription']).reset_index(drop=True)\n",
        "\n",
        "transcriptions = metadata['normalized_transcription'].astype(str)\n",
        "\n",
        "tokenizer = Tokenizer(char_level=True, filters='', lower=True)\n",
        "tokenizer.fit_on_texts(transcriptions)\n",
        "char_index = tokenizer.word_index\n",
        "vocab_size = len(char_index) + 1  # additioning a blank label to the vocab test\n",
        "\n",
        "print(f'Vocabulary Size (including blank): {vocab_size}')\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(transcriptions)\n",
        "\n",
        "# simple subtract 1\n",
        "sequences = [np.array(seq) - 1 for seq in sequences]\n",
        "\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "transcripts_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post', value=vocab_size - 1)\n",
        "\n",
        "print(f'Max Sequence Length: {max_seq_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy8qjShfu6b8",
        "outputId": "b001dd2f-647f-4cce-9bfd-fbcfc6c6cf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.5)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Vocabulary Size (including blank): 40\n",
            "Max Sequence Length: 185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#This whole process took over 12 times of running becasue I hit so many rb level\n",
        "#mismatches, blank label errors, issues with padding, and the CTC loss throwing errors\n",
        "\n",
        "\n",
        "def extract_features(file_name, max_time_steps=300):\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, sr=16000)\n",
        "        # for mmcc to prevent skewed data really affecting the ai model\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        mfccs = (mfccs - np.mean(mfccs)) / np.std(mfccs)\n",
        "        mfccs = mfccs.T\n",
        "        if mfccs.shape[0] > max_time_steps:\n",
        "            mfccs = mfccs[:max_time_steps, :]\n",
        "        else:\n",
        "            pad_width = max_time_steps - mfccs.shape[0]\n",
        "            mfccs = np.pad(mfccs, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_name}: {e}\")\n",
        "        return None\n",
        "    return mfccs\n",
        "\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "for index, row in metadata.iterrows():\n",
        "    file_name = f'/content/LJSpeech/LJSpeech-1.1/wavs/{row[\"file_name\"]}.wav'\n",
        "    features = extract_features(file_name)\n",
        "    if features is not None:\n",
        "        X.append(features)\n",
        "        Y.append(transcripts_padded[index])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "print(f'X shape: {X.shape}')\n",
        "print(f'Y shape: {Y.shape}')\n",
        "\n",
        "# simple splitting for 80-20 ratio\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'Y_train shape: {Y_train.shape}')\n",
        "print(f'X_val shape: {X_val.shape}')\n",
        "print(f'Y_val shape: {Y_val.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVl97JtavKVT",
        "outputId": "d3622937-f4ad-454c-9dbf-f284f372a3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (2986, 300, 40)\n",
            "Y shape: (2986, 185)\n",
            "X_train shape: (2388, 300, 40)\n",
            "Y_train shape: (2388, 185)\n",
            "X_val shape: (598, 300, 40)\n",
            "Y_val shape: (598, 185)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[2]\n",
        "time_steps = X_train.shape[1]\n",
        "output_dim = vocab_size\n",
        "\n",
        "inputs = Input(name='inputs', shape=(time_steps, input_dim))\n",
        "\n",
        "\n",
        "x = Bidirectional(GRU(256, return_sequences=True))(inputs)\n",
        "\n",
        "#i include this because it's helpful because context from both past and future sounds improves accuracy.\n",
        "\n",
        "x = Dropout(0.2)(x)\n",
        "x = Bidirectional(GRU(256, return_sequences=True))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Bidirectional(GRU(256, return_sequences=True))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "x = TimeDistributed(Dense(output_dim))(x)\n",
        "y_pred = Activation('softmax')(x)\n",
        "#Lets talk why speech data is strucutre.\n",
        "#The model tertmine the likelilhood of each character\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Model(inputs, y_pred)\n",
        "model.summary()\n",
        "\n",
        "# I customized the CTC loss to fit the vocabulary size and label index handling\n",
        "def ctc_loss_lambda_func(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, dtype='int32')\n",
        "\n",
        "    input_length = tf.fill([tf.shape(y_pred)[0], 1], tf.shape(y_pred)[1])\n",
        "\n",
        "    label_length = tf.reduce_sum(tf.cast(tf.not_equal(y_true, vocab_size - 1), dtype='int32'), axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "    return loss\n",
        "\n",
        "model.compile(optimizer='adam', loss=ctc_loss_lambda_func)\n",
        "\n",
        "Y_train_ctc = Y_train.astype('int32')\n",
        "Y_val_ctc = Y_val.astype('int32')\n",
        "\n",
        "assert np.max(Y_train_ctc) < vocab_size, \"Label index exceeds vocab size.\"\n",
        "assert np.max(Y_val_ctc) < vocab_size, \"Label index exceeds vocab size.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "Smj39-Q0vy6L",
        "outputId": "95f05784-d000-4c72-c341-e9cabd356d02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ inputs (\u001b[38;5;33mInputLayer\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m457,728\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_10 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_11 (\u001b[38;5;33mBidirectional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m1,182,720\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_5 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │          \u001b[38;5;34m20,520\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m40\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ inputs (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">457,728</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,182,720</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,520</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,843,688\u001b[0m (10.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,843,688</span> (10.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,843,688\u001b[0m (10.85 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,843,688</span> (10.85 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# very dislike on how dataset work because learning rate reduction callbacks for\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.00001)\n",
        "\n",
        "\n",
        "#I think if there is a example you can show us that would help alot\n",
        "#For CTC loss, what should we expect?\n",
        "\n",
        "#CTC represent the gaps between character but it kept causing errror.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.fit(X_train, Y_train_ctc,\n",
        "          batch_size=16,\n",
        "          epochs=30,\n",
        "          validation_data=(X_val, Y_val_ctc),\n",
        "          callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "def decode_predictions(pred):\n",
        "    input_length = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    #So here the line creates an array which tell function each prediction length\n",
        "    decoded, _ = tf.keras.backend.ctc_decode(pred, input_length, greedy=False, beam_width=10, top_paths=1)\n",
        "    #after we decoding the func return final\n",
        "    decoded = tf.keras.backend.get_value(decoded[0])\n",
        "    return decoded\n",
        "\n",
        "# beta test over 8 days of trying over and over\n",
        "def evaluate_model(model, X_val, Y_val, tokenizer):\n",
        "    predictions = model.predict(X_val)\n",
        "    decoded_outputs = decode_predictions(predictions)\n",
        "    decoded_sentences = []\n",
        "    actual_sentences = []\n",
        "    for i in range(len(decoded_outputs)):\n",
        "      #data structure sample:\n",
        "      #we put this loop which goes through each guess and answer one by one\n",
        "\n",
        "\n",
        "        decoded_sequence = [int(x) for x in decoded_outputs[i] if x != -1 and x != vocab_size - 1]\n",
        "\n",
        "        #decoded_sequence is the cleaned-up version of the model\n",
        "        #actual_sequence is the cleaned-up version of the actual answer text.\n",
        "        decoded_sentence = tokenizer.sequences_to_texts([np.array(decoded_sequence) + 1])[0]\n",
        "        decoded_sentences.append(decoded_sentence)\n",
        "        actual_sequence = [int(x) for x in Y_val[i] if x != vocab_size - 1]\n",
        "        actual_sentence = tokenizer.sequences_to_texts([np.array(actual_sequence) + 1])[0]\n",
        "        actual_sentences.append(actual_sentence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    total_cer = 0\n",
        "    for decoded_sentence, actual_sentence in zip(decoded_sentences, actual_sentences):\n",
        "        cer = wer(actual_sentence, decoded_sentence)\n",
        "        total_cer += cer\n",
        "    average_cer = total_cer / len(decoded_sentences)\n",
        "    print(f'Average Character Error Rate (CER): {average_cer * 100:.2f}%')\n",
        "    for i in range(5):\n",
        "        print(f'Decoded: {decoded_sentences[i]}')\n",
        "        print(f'Actual: {actual_sentences[i]}')\n",
        "        print('---')\n",
        "\n",
        "evaluate_model(model, X_val, Y_val, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXz7BtNAv6Qx",
        "outputId": "792d3ad9-c460-4c3a-9d71-d1bdfbdc79a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 281ms/step - loss: 368.3432 - val_loss: 229.9221 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 311ms/step - loss: 210.0755 - val_loss: 154.9332 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 269ms/step - loss: 153.1827 - val_loss: 120.3071 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 291ms/step - loss: 118.8342 - val_loss: 99.2141 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 290ms/step - loss: 100.1162 - val_loss: 87.7927 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 291ms/step - loss: 84.1063 - val_loss: 79.0900 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 290ms/step - loss: 73.5111 - val_loss: 74.5809 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 261ms/step - loss: 63.3928 - val_loss: 69.5143 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 261ms/step - loss: 56.5932 - val_loss: 65.8554 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 324ms/step - loss: 50.3235 - val_loss: 63.8464 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 282ms/step - loss: 44.6656 - val_loss: 62.4298 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 248ms/step - loss: 39.5978 - val_loss: 60.4809 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 285ms/step - loss: 35.0739 - val_loss: 60.4431 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 275ms/step - loss: 32.8553 - val_loss: 60.6003 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 293ms/step - loss: 29.0927 - val_loss: 60.7009 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 258ms/step - loss: 24.5309 - val_loss: 57.5668 - learning_rate: 5.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 290ms/step - loss: 20.2301 - val_loss: 58.1361 - learning_rate: 5.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 281ms/step - loss: 18.9891 - val_loss: 58.4798 - learning_rate: 5.0000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 269ms/step - loss: 16.8322 - val_loss: 57.8016 - learning_rate: 2.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 284ms/step - loss: 15.2393 - val_loss: 58.2164 - learning_rate: 2.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 290ms/step - loss: 14.2387 - val_loss: 58.2095 - learning_rate: 1.2500e-04\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 217ms/step\n",
            "Average Character Error Rate (CER): 16.44%\n",
            "Decoded: w e r e   e n d u c e d   t o   c o t   p u n c h e s   f o r   a s e a r i a s   o f   o l e d   s t y   l   l e t e r s .\n",
            "Actual: w e r e   i n d u c e d   t o   c u t   p u n c h e s   f o r   a   s e r i e s   o f   \" o l d   s t y l e \"   l e t t e r s .\n",
            "---\n",
            "Decoded: a n d   t h e r   s e m e s   t o   o   p e d   g o d   r e a s a n   f o r   s u p o s e n   t h a t   h e   w a s   a   g r e a t e r   v i l o n d   b e n   a n y   o f   t h o s e   a r a i n e d .\n",
            "Actual: a n d   t h e r e   s e e m s   t o   h a v e   b e e n   g o o d   r e a s o n   f o r   s u p p o s i n g   t h a t   h e   w a s   a   g r e a t e r   v i l l a i n   t h a n   a n y   o f   t h o s e   a r r a i g n e d .\n",
            "---\n",
            "Decoded: w h i l e   t h e   c a p i t a l   c o n v i c t s ,   w h o   w e r e   l a t e l y   i n   t h a t   b l a c k   p e w ,   a p   p e r   f a n t   w i t h   h e   m o t i o n .\n",
            "Actual: w h i l e   t h e   c a p i t a l   c o n v i c t s   w h o   w e r e   l a t e l y   i n   t h a t   b l a c k   p e w   a p p e a r   f a i n t   w i t h   e m o t i o n .\n",
            "---\n",
            "Decoded: t h e   c r o u d   b e   a n   t o c o n g r e g a t e   i n   o n   a b o u t   t h e   o l d   b a n g l y .\n",
            "Actual: t h e   c r o w d   b e g a n   t o   c o n g r e g a t e   i n   a n d   a b o u t   t h e   o l d   b a i l e y .\n",
            "---\n",
            "Decoded: h e   l e t d   o f   t h e   c a u s   t e   a s o m   a g e   i n t e   f e r d w i t h   a n   l a f   t h a t ,   a n d   f o r m   s e g   t h a t   n o   s i x c e s   t a t e n d   i t   i s   e f o r t s ,   o w i n g   t o   t h e   e v i l s   a r i s i n g   f o r   a s o c i a .\n",
            "Actual: h e   l e f t   o f f   b e c a u s e   h e   w a s   s o   m u c h   i n t e r f e r e d   w i t h   a n d   l a u g h e d   a t ,   a n d   f r o m   s e e i n g   t h a t   n o   s u c c e s s   a t t e n d e d   h i s   e f f o r t s ,   o w i n g   t o   t h e   e v i l s   a r i s i n g   f r o m   a s s o c i a t i o n .\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}